{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4af339",
   "metadata": {},
   "source": [
    "# Negative Potential Vorticity Interactions with the Jet Stream Algorithm\n",
    "#### Author: Alexander Lojko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa38779",
   "metadata": {},
   "source": [
    "## Section 1: Import Data and Prepare Filtering Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac73e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Required Packages: Numpy, xarray, itertools, scipy, metpy, geopy and skimage\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import itertools \n",
    "\n",
    "import metpy as metpy\n",
    "import metpy.xarray as mx\n",
    "import metpy.calc as mc\n",
    "\n",
    "import skimage\n",
    "from skimage import feature\n",
    "from skimage import measure\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import label, generate_binary_structure\n",
    "\n",
    "import geopy.distance\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e63877ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [86], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Step 1: Import PV Data, Concat. data: \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Open PV data (code assumes using multiple years of data saved seperately)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m data_pv \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mby_coords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m### Step 2: Rename data variables to be more intuitive: \u001b[39;00m\n\u001b[1;32m      8\u001b[0m data_pv \u001b[38;5;241m=\u001b[39m data_pv\u001b[38;5;241m.\u001b[39mrename({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPV_GDS0_ISBL\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpv\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_time0_hours\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg0_lat_1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg0_lon_2\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/api.py:937\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mfspath(p) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, os\u001b[38;5;241m.\u001b[39mPathLike) \u001b[38;5;28;01melse\u001b[39;00m p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno files to open\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(concat_dim, (\u001b[38;5;28mstr\u001b[39m, DataArray)) \u001b[38;5;129;01mor\u001b[39;00m concat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: no files to open"
     ]
    }
   ],
   "source": [
    "### Step 1: Import PV Data, Concat. data: \n",
    "\n",
    "# Open PV data (code assumes using multiple years of data saved seperately)\n",
    "data_pv = xr.open_mfdataset('...',combine = 'by_coords', chunks=None)\n",
    "\n",
    "### Step 2: Rename data variables to be more intuitive: \n",
    "\n",
    "data_pv = data_pv.rename({'PV_GDS0_ISBL':'pv','initial_time0_hours':'time','g0_lat_1': 'latitude','g0_lon_2':'longitude'})\n",
    "\n",
    "data_pv = data_pv.sel(latitude=slice(80., 10.),longitude=slice(-120, -30))    # For West-Atlantic Analysis\n",
    "\n",
    "### Step 3: Select PV variable and convert to PVU\n",
    "\n",
    "data_pv = data_pv.pv*10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc7a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Import PV Data, Concat. data: \n",
    "\n",
    "# pv data:\n",
    "data_pv = xr.open_mfdataset('/scratch/aepayne_root/aepayne0/alojko/Proj2_Extra_PV_05_int/era5_*.nc4',combine = 'by_coords', chunks=None)\n",
    "\n",
    "### Step 2: Rename data variables to be more intuitive: \n",
    "\n",
    "data_pv = data_pv.rename({'PV_GDS0_ISBL':'pv','initial_time0_hours':'time','g0_lat_1': 'latitude','g0_lon_2':'longitude'})\n",
    "\n",
    "data_pv = data_pv.sel(latitude=slice(80., 10.),longitude=slice(-120, -30), time=slice('2017-06-05', '2017-06-20'))    # For West-Atlantic Analysis\n",
    "\n",
    "### Step 3: Select PV variable and convert to PVU\n",
    "\n",
    "data_pv = data_pv.pv*10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df53b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Define Constants for calculations below:\n",
    "\n",
    "# Define Domain to search for Negative PV-Jet Interactions: I use [65, 25N - 100, 50W]\n",
    "lat_max = 65.\n",
    "lat_min = 25.\n",
    "lon_min = -100\n",
    "lon_max = -50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fc1a2",
   "metadata": {},
   "source": [
    "## Section 2: Identification and Length-scale Calculations of Negative PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910cd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Filter PVU for > -0.01 PVU: Seek negative PV\n",
    "data_pv_binary = data_pv.where(data_pv >= -0.01, 1).where(data_pv < -0.01, 0)\n",
    "\n",
    "### Defining 3d array for labelling: \n",
    "\n",
    "# Step 6: Use ndimage to identification of negative PV objects. \n",
    "rgiObj_Struct2D = np.zeros((3,3,3)); rgiObj_Struct2D[1,:,:]=1\n",
    "data_pv_labeled, num_features = label(data_pv_binary, structure=rgiObj_Struct2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9ceb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Also calculate the area size of each negative PV label. OPTIONAL STEP:\n",
    "\n",
    "## Creation of area size mesh-grid: \n",
    "\n",
    "lons_mesh, lats_mesh = np.meshgrid(data_pv_binary.longitude, data_pv_binary.latitude)\n",
    "\n",
    "EarthCircum = 40075 #[km]\n",
    "dLat = np.copy(lons_mesh)\n",
    "dLat[:] = EarthCircum/(360/(lats_mesh[1,0]-lats_mesh[0,0]))\n",
    "dLon = np.copy(lons_mesh)\n",
    "\n",
    "for la in range(lats_mesh.shape[0]):\n",
    "    dLon[la,:] = EarthCircum/(360/(lats_mesh[1,0]-lats_mesh[0,0]))*np.cos(np.deg2rad(lats_mesh[la,0]))\n",
    "\n",
    "Gridspacing = np.mean(np.append(dLat[:,:,None],dLon[:,:,None], axis=2))\n",
    "Area = dLat*dLon\n",
    "Area[Area < 0] = 0\n",
    "\n",
    "### We have now calculated area in meters for each grid-cell (latitude weighted)\n",
    "\n",
    "# Only calculate the area size of negative PV feature: (Can be used as alternative to major-axis length criteria)\n",
    "\n",
    "Area_time = np.repeat(Area[np.newaxis, :, :], np.size(data_pv_binary.time), axis=0)\n",
    "Area_time_pv = np.where(data_pv_binary.values==0, data_pv_binary.values, Area_time)\n",
    "\n",
    "data_pv_labeled_areas = np.array(ndimage.sum(Area_time_pv, data_pv_labeled, np.arange(data_pv_labeled.max()+1)))\n",
    "\n",
    "data_pv_labeled_areas = data_pv_labeled_areas[1:] # We calculate area coverage of each label here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cb81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Calculate the length-scales of each negative PV feature. \n",
    "# Initialize dictionaries to store start and end points and label lengths\n",
    "data_pv_labeled_lengths = []\n",
    "\n",
    "# Iterate through each label and find start and end points\n",
    "for label_id in range(1, num_features + 1):\n",
    "    label_mask = (data_pv_labeled == label_id)\n",
    "    \n",
    "    # Get the indices (time, latitude, longitude) of the label\n",
    "    indices = np.argwhere(label_mask)\n",
    "    \n",
    "    # Extract latitude and longitude values\n",
    "    latitudes = indices[:, 1]\n",
    "    longitudes = indices[:, 2]\n",
    "    \n",
    "    # Find the start and end points by finding the minimum and maximum latitude and longitude\n",
    "    start_lat = latitudes.min()\n",
    "    end_lat = latitudes.max()\n",
    "    start_lon = longitudes.min()\n",
    "    end_lon = longitudes.max()\n",
    "    \n",
    "    # Convert latitude and longitude values to coordinates\n",
    "    start_coords = (data_pv.coords['latitude'][start_lat].values, data_pv.coords['longitude'][start_lon].values)\n",
    "    end_coords = (data_pv.coords['latitude'][end_lat].values, data_pv.coords['longitude'][end_lon].values)\n",
    "    \n",
    "    # Calculate the great circle distance of negative PV length (i.e., account for earth curvature)\n",
    "    length_km = geodesic(start_coords, end_coords).kilometers\n",
    "    data_pv_labeled_lengths.append(length_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914bcef",
   "metadata": {},
   "source": [
    "## Section 3: First Part of Filtering Negative PV: Data Processing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc323f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labeled_array into xarray to keep track of coordinates if needed:\n",
    "data_pv_labeled_xr = xr.zeros_like(data_pv) + data_pv_labeled\n",
    "\n",
    "# Use defined domain mask to identify negative PV features overlapping with domain.\n",
    "\n",
    "domain_mask = (data_pv_labeled_xr.coords['latitude'] >= lat_min) & (data_pv_labeled_xr.coords['latitude'] <= lat_max) & \\\n",
    "              (data_pv_labeled_xr.coords['longitude'] >= lon_min) & (data_pv_labeled_xr.coords['longitude'] <= lon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779a0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "### New section, create a list of latlon which is indexed by label number: \n",
    "\n",
    "data_pv_labeled_xr_nans = data_pv_labeled_xr.where(data_pv_labeled_xr!=0)\n",
    "data_pv_labeled_xr_stacked = data_pv_labeled_xr_nans.stack(coordinates=['time', 'latitude','longitude']) \n",
    "\n",
    "### Remove the NaNs (or zeros):\n",
    "\n",
    "data_pv_labeled_xr_stacked = data_pv_labeled_xr_stacked[data_pv_labeled_xr_stacked.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da5d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove repeating instances of negative PV labels: \n",
    "\n",
    "index_no, time_index = np.unique(data_pv_labeled_xr_stacked.values, return_index=True)\n",
    "data_pv_label_time = data_pv_labeled_xr_stacked.isel(coordinates=time_index).time.to_numpy() # each labels time\n",
    "\n",
    "# use Counter to count occurrences of each date\n",
    "date_counts = Counter(data_pv_label_time)\n",
    "\n",
    "# create new array with counts of each unique date: How many NPV labels occur for a particular time-step? \n",
    "unique_dates = list(set(data_pv_label_time))\n",
    "unique_dates = np.sort(unique_dates)\n",
    "data_pv_count_array = [date_counts[date] for date in unique_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337a6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the values in the initial Xarray based on the repeat_counts array:\n",
    "\n",
    "repeated_pv_data = np.repeat(data_pv.values, data_pv_count_array, axis=0)\n",
    "\n",
    "# Create a new 3D Xarray with repeated indexes along the 'x' dimension\n",
    "repeated_pv_data = xr.DataArray(repeated_pv_data, dims=('time', 'latitude', 'longitude'))\n",
    "\n",
    "repeated_pv_label = np.repeat(data_pv_labeled_xr_nans.values, data_pv_count_array, axis=0)\n",
    "repeated_pv_label = xr.DataArray(repeated_pv_label, dims=('time', 'latitude', 'longitude')) \n",
    "\n",
    "repeated_pv_label_number = repeated_pv_label\n",
    "repeated_pv_label_number['time'] = repeated_pv_label_number['time'] + 1 # set times to match labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6723ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we have repeated label array, now doing the same thing for domain mask array:\n",
    "\n",
    "# Get the number of time steps (size of the time dimension) in the 3D xarray\n",
    "num_time_steps = repeated_pv_label.sizes['time']\n",
    "\n",
    "# Repeat the 2D xarray along the new 'time' dimension\n",
    "repeated_mask = xr.concat([domain_mask] * num_time_steps, dim='time')\n",
    "\n",
    "# Assign coordinates from the 3D xarray to the new 'time' dimension\n",
    "repeated_mask['time'] = repeated_pv_label.coords['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1adf6ba",
   "metadata": {},
   "source": [
    "## Section 4: Filter out Negative PV features not inside domain of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5974057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next step:\n",
    "# for repeated_pv_label, where nan, set to 0, where number set to 1\n",
    "# for repeated_mask, where False, set to 0, where True, set to 1\n",
    "# Then, add the 2 matrix up, for each label (time) where a value of 2 appears, we keep. \n",
    "\n",
    "repeated_pv_label = repeated_pv_label.fillna(0)\n",
    "repeated_pv_label = repeated_pv_label.where(repeated_pv_label <= 1, 1)\n",
    "\n",
    "repeated_mask = repeated_mask.astype(int)\n",
    "repeated_mask['time'] = repeated_mask['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "406c7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let's add up the 2 matrix and search for values of 2 per label\n",
    "\n",
    "label_mask_filter = repeated_pv_label + repeated_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e966756",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We now have values that add up: \n",
    "label_mask_filter_max = []\n",
    "\n",
    "for i in range(label_mask_filter.shape[0]):\n",
    "    label_mask_filter_max.append(np.max(label_mask_filter[i,:,:].values))\n",
    "    \n",
    "repeated_pv_label_number = repeated_pv_label_number.fillna(0)\n",
    "time_labels = np.arange(1,np.size(repeated_pv_label_number.time)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a3c9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section searches for negative PV features inside the domain by seeking values of 2: Where there is 2 means NPV label overlaps with domain.\n",
    "individual_pv_labels = xr.zeros_like(repeated_pv_label_number)\n",
    "\n",
    "# Iterate through each time step\n",
    "for time_idx in range(len(repeated_pv_label_number.time)):\n",
    "    label_filter = time_labels[time_idx]\n",
    "    \n",
    "    # Create a boolean mask to select the label for the current time step\n",
    "    label_mask = repeated_pv_label_number.sel(time=repeated_pv_label_number.time[time_idx]) == label_filter\n",
    "    \n",
    "    # Set the selected label in the filtered data, and all other labels to zero\n",
    "    individual_pv_labels[time_idx] = label_mask.astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d68e701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick check to make sure still using lat/lon coords rather than numpy (each label has its own time-dimension)\n",
    "\n",
    "individual_pv_labels['latitude'] = data_pv.coords['latitude']\n",
    "individual_pv_labels['longitude'] = data_pv.coords['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd1536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check where NPV label intersects with domain:\n",
    "\n",
    "pv_label_mask_filter = individual_pv_labels + repeated_mask\n",
    "\n",
    "# Create a new 1D vector with the same size as the time dimension, initialized with zeros\n",
    "new_vector = xr.DataArray(np.zeros(pv_label_mask_filter.time.size), dims='time', coords={'time': pv_label_mask_filter.time})\n",
    "\n",
    "# Check if 2 occurs in the 3D xarray along the time dimension\n",
    "presence_of_2 = (pv_label_mask_filter == 2).any(dim=('latitude', 'longitude'))\n",
    "\n",
    "# Assign 1 to the new vector for time steps where 2 occurs\n",
    "new_vector[presence_of_2] = 1\n",
    "\n",
    "### We now have the index where labels overlap with the mask domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a59a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_label = individual_pv_labels.sel(time=new_vector == 1) # labeled arrays\n",
    "selected_pv = repeated_pv_data.sel(time=new_vector == 1) # regular pv field\n",
    "selected_time_index_labels = data_pv_label_time[new_vector==1] # times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb8dc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lengths_km_filtered = np.array(data_pv_labeled_lengths)[new_vector.values.astype(int) == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57619fd",
   "metadata": {},
   "source": [
    "## Section 5: Filter out small negative PV features, only keep synoptic scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07aa4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 98th percentile, where the 98th percentile is treated as synoptic-scale. \n",
    "# In Lojko et al., 2024, climatological length scale in ERA5 is about 1650 km. For now, let's just keep upper 98th percentile.\n",
    "\n",
    "percentile_threshold = np.percentile(label_lengths_km_filtered, 98)\n",
    "\n",
    "# Let's filter at percentile threshold of length scale.  \n",
    "\n",
    "percentile_label_length_index = np.where(label_lengths_km_filtered > percentile_threshold)\n",
    "\n",
    "label_lengths_km_filtered_percentile = label_lengths_km_filtered[percentile_label_length_index]\n",
    "selected_data_percentile = selected_pv.where(np.isin(selected_pv,  percentile_label_length_index), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e891794",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preperation: \n",
    "\n",
    "selected_label_filt = selected_label.isel(time=np.vstack(percentile_label_length_index).squeeze())\n",
    "selected_pv_filt = selected_pv.isel(time=np.vstack(percentile_label_length_index).squeeze())\n",
    "\n",
    "### Make xr like to use isel.\n",
    "\n",
    "selected_time_index_labels_filt = selected_time_index_labels[np.vstack(percentile_label_length_index).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e6fcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstruct the selected_pv_filt dataset as before, now make time axis instead of axis being index of label. \n",
    "selected_pv_filt['latitude'] = selected_label_filt.coords['latitude']\n",
    "selected_pv_filt['longitude'] = selected_label_filt.coords['longitude']\n",
    "selected_pv_filt['time'] = selected_time_index_labels_filt\n",
    "\n",
    "selected_label_filt['time'] = selected_time_index_labels_filt # recently added in, removes index of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfc19158",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relabel remaining labels from 1 - N:\n",
    "unique_selected_label_filt = selected_label_filt.groupby('time').first()\n",
    "\n",
    "# Restructuring of time data:\n",
    "labeled_array_new, num_features_new = label(selected_label_filt, structure=rgiObj_Struct2D)\n",
    "labeled_array_new_xr = xr.zeros_like(selected_label_filt) + labeled_array_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17fd9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First some re-stacking of labeled data filtered:\n",
    "### New section, create a list of latlon which is indexed by label number: \n",
    "xr_remove_small_label_filt_nans = labeled_array_new_xr.where(labeled_array_new_xr!=0)\n",
    "xr_remove_small_label_stacked = xr_remove_small_label_filt_nans.stack(coordinates=['time', 'latitude','longitude']) \n",
    "\n",
    "### Remove the NaNs (or zeros):\n",
    "xr_remove_small_label_stacked = xr_remove_small_label_stacked[xr_remove_small_label_stacked.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e6800f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Last step, get a list of coordinates of all Neg PV labels remaining:\n",
    "\n",
    "lats_list = [] \n",
    "lons_list = []\n",
    "times_list = [] \n",
    "\n",
    "for i in range(int(np.max(xr_remove_small_label_stacked.to_numpy()+1))):\n",
    "    lats_list.append(xr_remove_small_label_stacked['latitude'].where(xr_remove_small_label_stacked == i, drop=True).to_numpy())\n",
    "    lons_list.append(xr_remove_small_label_stacked['longitude'].where(xr_remove_small_label_stacked == i, drop=True).to_numpy())\n",
    "\n",
    "### Combine lats and lons: \n",
    "\n",
    "listing_label_1PVU = []   # recycling the same name as before. But this time with corrected coordinates: \n",
    "for a, b in zip(lats_list, lons_list):\n",
    "    listing_label_1PVU.append(list(zip(a,b)))\n",
    "\n",
    "listing_label_1PVU = listing_label_1PVU[1:] # first index is no value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974805cb",
   "metadata": {},
   "source": [
    "### Section 6: Preperation of 2 PVU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0359c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2PVU time:\n",
    "\n",
    "data_pv_smooth = selected_pv_filt # Filter times to match -1 PVU days of interest. \n",
    "data_pv_smooth = mc.smooth_gaussian(data_pv_smooth, 15)  # Smooth to reduce instances of very small 2 pvu filaments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29a889fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use scikit image package to ID lines of 2 PVU: (MUST BE A LIST!)\n",
    "\n",
    "listing_2PVU = []\n",
    "listing_2PVU_filt = [] # This list will filter out closed contours.\n",
    "\n",
    "for i in range(np.size(data_pv_smooth.time)): # time size should be the same for pv_pos_2\n",
    "    listing_2PVU.append(measure.find_contours(data_pv_smooth.values[i], 2))\n",
    "    listing_2PVU_filt.append([c for c in listing_2PVU[i] if c[0,0] != c[-1,0] and c[0,1] != c[-1,1]])\n",
    "    \n",
    "### Concatenate the middle nested list axis (contour number) due to scikit image seperating individual contours: \n",
    "\n",
    "listing_2PVU_filt = [list(itertools.chain(*sub)) for sub in listing_2PVU_filt]\n",
    "\n",
    "### At this point, we have the coordinates, but they will need to be transformed back onto the lat-lon grid. This will be \n",
    "### done shortly. However, there is a small subset of dates that have no unclosed contours identified. When calculating\n",
    "### the distance between -1 PVU coords and 2 PVU coords, there needs to be at least 1 coordinate for calculation to work.\n",
    "### Hence, first, let us first filter out any time-steps in the 2 PVU list with no values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ff8930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alojko/miniconda3/envs/cartopy-pangeo/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3202: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).size\n"
     ]
    }
   ],
   "source": [
    "### Creating script prior to Jet - to - negative PV feature detection, eliminate all instances of 0 data size in list. \n",
    "\n",
    "size_append_2PVU = [] \n",
    "for i in range(np.size(listing_2PVU_filt)):\n",
    "    size_append_2PVU.append(np.size(listing_2PVU_filt[i])) # Only 2 PVU list has instances of 0 data. \n",
    "\n",
    "zero_index_2PVU = [] # indicies where value == 0\n",
    "for i, x in enumerate(size_append_2PVU):\n",
    "    if x == 0:\n",
    "        zero_index_2PVU.append(i)\n",
    "        \n",
    "zero_index_2PVU_label = list(np.asarray(zero_index_2PVU) + 1)\n",
    "        \n",
    "non_zero_index_2PVU = [] # indicies where value != 0\n",
    "for i, x in enumerate(size_append_2PVU):\n",
    "    if x != 0:\n",
    "        non_zero_index_2PVU.append(i)\n",
    "        \n",
    "non_zero_index_2PVU_label = list(np.asarray(non_zero_index_2PVU) + 1)\n",
    "\n",
    "### Next step, filter out the ID'd zero_index_2PVU in  the 2 PVU list, -1 PVU list and in time axis. \n",
    "\n",
    "listing_neg_1_pv_filt = [x for i, x in enumerate(listing_label_1PVU) if i not in zero_index_2PVU]\n",
    "listing_pos_2_pv_filt = [x for i, x in enumerate(listing_2PVU_filt) if i not in zero_index_2PVU]\n",
    "\n",
    "#np.shape(listing_test_2_filt)\n",
    "\n",
    "size_append_2PVUnew = [] \n",
    "for i in range(np.size(listing_neg_1_pv_filt)):\n",
    "    size_append_2PVUnew.append(np.size(listing_pos_2_pv_filt[i])) # Only 2 PVU list has instances of 0 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2591308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep track of seperate time array, make sure has same index as the -1 PVU and 2 PVU list we are making:\n",
    "repeated_time_nonzero = selected_time_index_labels_filt[non_zero_index_2PVU]\n",
    "\n",
    "# In order to change lat/lon, need to vstack to reduce instances of 'array' in nested list: (repeat process)  \n",
    "\n",
    "listing_2PVU_clean =[]\n",
    "for i in range(np.size(listing_pos_2_pv_filt)): \n",
    "    listing_2PVU_clean.append(np.vstack(listing_pos_2_pv_filt[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1953c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate transformation from numpy index to lat/lon: \n",
    "\n",
    "lons_2PVU = []\n",
    "lats_2PVU = [] \n",
    "coord = [] \n",
    "\n",
    "for contour in listing_2PVU_clean:\n",
    "    lons_2PVU.append(data_pv.longitude.values[0]+contour[:, 1]*0.5)  # 0.5 because of reslution of 2PVU line used. \n",
    "    lats_2PVU.append(data_pv.latitude.values[0]+contour[:, 0]*-0.5)\n",
    "\n",
    "# FOR LON: -(MOST WEST LONGITUDE)+CONTOUR*(GRID RESOLUTION)      --- assuming longitude is negative. \n",
    "# FOR LAT: +(MOST NORTH LATITUDE)+CONTOUR*(-GRID RESOLUTION)    --- assuming positive latitude, note negative sign in grid res. \n",
    "\n",
    "listing_2PVU_final = []   # recycling the same name as before. But this time with corrected coordinates: \n",
    "for a, b in zip(lats_2PVU, lons_2PVU):\n",
    "    listing_2PVU_final.append(list(zip(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1ed91",
   "metadata": {},
   "source": [
    "### Section 7: Find minimum distance of each negative PV label to a continious 2 PVU contour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "760e4449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  29.045618513599038\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dist = [] \n",
    "coord_append = [] \n",
    "min_dist = [] \n",
    "min_index = [] \n",
    "coord_append_min = [] \n",
    "\n",
    "for i in range(np.size(listing_2PVU_final)): # Time Axis \n",
    "    for value1 in listing_2PVU_final[i]: # For each coordinate pair in PV2\n",
    "            for value2 in listing_neg_1_pv_filt[i]:  # For each coordinate pair in negPV1\n",
    "                dist.append(geopy.distance.great_circle(value1, value2).km) # Calculate distance \n",
    "                coord_append.append([value1, value2]) # all coordinates appended for each time-step. \n",
    "    min_dist.append(np.min(dist)) # Find minimum distance at each time. \n",
    "    min_index.append(np.argmin(dist))\n",
    "    coord_append_min.append(coord_append[min_index[i]]) # set to i to find minimum mid-point at each time-step. \n",
    "    dist = []  # Clear distance array to save memory and for minimum distance func. to work. \n",
    "    coord_append = [] \n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12294abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make into numpy array:\n",
    "\n",
    "min_dist_array = np.array(min_dist)\n",
    "min_index_array = np.array(min_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b0107c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering values not within 100 km:\n",
    "\n",
    "min_dist_keep = np.where(min_dist_array <= 100) \n",
    "min_dist_keep = min_dist_keep[0]\n",
    "\n",
    "min_dist_discard = np.where(min_dist_array >= 100) \n",
    "min_dist_discard = min_dist_discard[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cce2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Also get mid-points:\n",
    "midpoint_coords = []\n",
    "just_2PVU = []\n",
    "just_1PVU = []\n",
    "for i in range(np.size(listing_2PVU_final)): # Time Axis\n",
    "    just_2PVU.append(np.array(coord_append_min[i][0]))\n",
    "    just_1PVU.append(np.array(coord_append_min[i][1]))\n",
    "    midpoint_coords.append((np.array(coord_append_min[i][0]) + np.array(coord_append_min[i][1]) )/2)\n",
    "    \n",
    "midpoint_coords = np.vstack(midpoint_coords)\n",
    "\n",
    "midpoint_coords_100 = midpoint_coords[min_dist_keep]\n",
    "min_dist_100 = np.array(min_dist)[min_dist_keep]\n",
    "repeated_time_nonzero_100 = repeated_time_nonzero[min_dist_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc01f3c",
   "metadata": {},
   "source": [
    "## Final filtering, if multiple NPV features near jet stream for single time-step, only keep one closest to the jet stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2c4c174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now check \n",
    "# Define your conditions\n",
    "lat_condition = (midpoint_coords_100[:, 0] >= lat_min) & (midpoint_coords_100[:, 0] <= lat_max)\n",
    "lon_condition = (midpoint_coords_100[:, 1] >= lon_min) & (midpoint_coords_100[:, 1] <= lon_max)\n",
    "\n",
    "# Apply the conditions\n",
    "filtered_midpoint_coords_100 = midpoint_coords_100[lat_condition & lon_condition]\n",
    "\n",
    "# Also get index of where condition is satisfied:\n",
    "condition_satisfied = lat_condition & lon_condition\n",
    "\n",
    "# Get indices of coordinates that satisfy the conditions\n",
    "indices = np.where(condition_satisfied)[0]\n",
    "\n",
    "# Apply indice to the -1PVU and 2PVU coords:\n",
    "just_1PVU_filt = np.vstack(just_1PVU)[indices]\n",
    "just_2PVU_filt = np.vstack(just_2PVU)[indices]\n",
    "\n",
    "# Also, to the remaining times:\n",
    "repeated_time_nonzero_filt = repeated_time_nonzero_100[indices]\n",
    "min_dist_100_filt = min_dist_100[indices]\n",
    "midpoint_coords_100_filt = midpoint_coords_100[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84aa8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter out repeating dates:\n",
    "\n",
    "# Combine dates and numbers using zip\n",
    "combined_data = list(zip(repeated_time_nonzero_filt, min_dist_100_filt))\n",
    "\n",
    "# Create a dictionary to store the minimum number for each unique date\n",
    "min_numbers = {}\n",
    "for date, distance in combined_data:\n",
    "    if date not in min_numbers or distance < min_numbers[date]:\n",
    "        min_numbers[date] = distance\n",
    "\n",
    "# Create filtered arrays\n",
    "filtered_dates, filtered_numbers = zip(*min_numbers.items())\n",
    "\n",
    "# Convert the filtered data to numpy arrays\n",
    "filtered_dates = np.array(filtered_dates)\n",
    "filtered_numbers = np.array(filtered_numbers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cartopy-Pangeo",
   "language": "python",
   "name": "cartopy-pangeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
